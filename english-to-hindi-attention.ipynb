{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1355361,"sourceType":"datasetVersion","datasetId":789090}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport re\nimport string\nfrom sklearn.model_selection import train_test_split\n\n# Load and preprocess the dataset\nlines = pd.read_csv('/kaggle/input/hindi-english-parallel-corpus/hindi_english_parallel.csv')\nlines = lines[:30000]\n\n# Lowercase all characters\nlines['english'] = lines['english'].apply(lambda x: str(x).lower().strip())\nlines['hindi'] = lines['hindi'].apply(lambda x: str(x).lower().strip())\n\n# Remove quotes and special characters\nlines['english'] = lines['english'].apply(lambda x: re.sub(\"'\", '', x))\nlines['hindi'] = lines['hindi'].apply(lambda x: re.sub(\"'\", '', x))\nexclude = set(string.punctuation)\nlines['english'] = lines['english'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\nlines['hindi'] = lines['hindi'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n\n# Remove digits\nremove_digits = str.maketrans('', '', string.digits)\nlines['english'] = lines['english'].apply(lambda x: x.translate(remove_digits))\nlines['hindi'] = lines['hindi'].apply(lambda x: x.translate(remove_digits))\n\n# Remove extra spaces and add start and end tokens to target sequences\nlines['english'] = lines['english'].apply(lambda x: re.sub(\" +\", \" \", x))\nlines['hindi'] = lines['hindi'].apply(lambda x: re.sub(\" +\", \" \", x))\nlines['hindi'] = lines['hindi'].apply(lambda x: 'START_ ' + x + ' _END')\n\n# Display a few examples to check preprocessing\nprint(lines.head())\n\n\n# Split the data into train, validation, and test sets\nX, y = lines['english'], lines['hindi']\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\n# Build vocabulary\nall_eng_words = set(word for eng in X_train for word in eng.split())\nall_hindi_words = set(word for hin in y_train for word in hin.split())\n\n# Create word to index and index to word dictionaries for both languages\ninput_token_index = {word: i + 1 for i, word in enumerate(sorted(all_eng_words))}\ntarget_token_index = {word: i + 1 for i, word in enumerate(sorted(all_hindi_words))}\nreverse_input_token_index = {i: word for word, i in input_token_index.items()}\nreverse_target_token_index = {i: word for word, i in target_token_index.items()}\n\n# Set the maximum sequence lengths\nmax_length_src = max(len(eng.split()) for eng in X_train)\nmax_length_tar = max(len(hin.split()) for hin in y_train)\n\n# Print stats\nprint(f\"Max length of English Sentence: {max_length_src}\")\nprint(f\"Max length of Hindi Sentence: {max_length_tar}\")\nprint(f\"Total English Words: {len(input_token_index)}\")\nprint(f\"Total Hindi Words: {len(target_token_index)}\")\n\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\nclass TranslationDataset(Dataset):\n    def __init__(self, X, y, input_token_index, target_token_index, max_length_src, max_length_tar):\n        self.X = X\n        self.y = y\n        self.input_token_index = input_token_index\n        self.target_token_index = target_token_index\n        self.max_length_src = max_length_src\n        self.max_length_tar = max_length_tar\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        input_seq = self.X.iloc[idx]\n        target_seq = self.y.iloc[idx]\n\n        # Tokenize and pad/truncate sequences\n        encoder_input_data = torch.zeros(self.max_length_src, dtype=torch.long)\n        decoder_input_data = torch.zeros(self.max_length_tar, dtype=torch.long)\n        decoder_target_data = torch.zeros(self.max_length_tar, dtype=torch.long)\n\n        # Populate encoder_input_data with the input sequence tokens\n        for t, word in enumerate(input_seq.split()[:self.max_length_src]):  # Truncate if longer than max_length_src\n            encoder_input_data[t] = self.input_token_index.get(word, 0)  # 0 for unknown words\n\n        # Populate decoder_input_data and decoder_target_data with the target sequence tokens\n        target_words = target_seq.split()[:self.max_length_tar]  # Truncate if longer than max_length_tar\n        for t, word in enumerate(target_words):\n            decoder_input_data[t] = self.target_token_index.get(word, 0)  # 0 for unknown words\n            if t > 0:\n                decoder_target_data[t - 1] = self.target_token_index.get(word, 0)\n\n        return encoder_input_data, decoder_input_data, decoder_target_data\n\n# Create DataLoader for train, validation, and test\ntrain_dataset = TranslationDataset(X_train, y_train, input_token_index, target_token_index, max_length_src, max_length_tar)\nval_dataset = TranslationDataset(X_val, y_val, input_token_index, target_token_index, max_length_src, max_length_tar)\ntest_dataset = TranslationDataset(X_test, y_test, input_token_index, target_token_index, max_length_src, max_length_tar)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-08-28T16:56:17.972100Z","iopub.execute_input":"2024-08-28T16:56:17.972715Z","iopub.status.idle":"2024-08-28T16:56:33.957801Z","shell.execute_reply.started":"2024-08-28T16:56:17.972669Z","shell.execute_reply":"2024-08-28T16:56:33.956854Z"},"trusted":true},"outputs":[{"name":"stdout","text":"                                               hindi  \\\n0  START_ अपने अनुप्रयोग को पहुंचनीयता व्यायाम का...   \n1        START_ एक्सेर्साइसर पहुंचनीयता अन्वेषक _END   \n2   START_ निचले पटल के लिए डिफोल्ट प्लगइन खाका _END   \n3    START_ ऊपरी पटल के लिए डिफोल्ट प्लगइन खाका _END   \n4  START_ उन प्लगइनों की सूची जिन्हें डिफोल्ट रूप...   \n\n                                          english  \n0  give your application an accessibility workout  \n1               accerciser accessibility explorer  \n2  the default plugin layout for the bottom panel  \n3     the default plugin layout for the top panel  \n4  a list of plugins that are disabled by default  \nMax length of English Sentence: 62\nMax length of Hindi Sentence: 46\nTotal English Words: 2518\nTotal Hindi Words: 3208\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-24T20:19:19.853705Z","iopub.execute_input":"2024-08-24T20:19:19.854021Z","iopub.status.idle":"2024-08-24T20:19:20.212338Z","shell.execute_reply.started":"2024-08-24T20:19:19.853988Z","shell.execute_reply":"2024-08-24T20:19:20.211341Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/hindi-english-parallel-corpus/hindi_english_parallel.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Encoder(nn.Module):\n    def __init__(self, input_dim, emb_dim, hidden_dim, n_layers, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(input_dim, emb_dim)\n        self.lstm = nn.LSTM(emb_dim, hidden_dim, n_layers, dropout=dropout, bidirectional=True)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, src):\n        embedded = self.dropout(self.embedding(src))\n        outputs, (hidden, cell) = self.lstm(embedded)\n        return outputs, hidden, cell\n\nclass Attention(nn.Module):\n    def __init__(self, hidden_dim):\n        super().__init__()\n        self.attn = nn.Linear(hidden_dim * 3, hidden_dim)\n        self.v = nn.Linear(hidden_dim, 1, bias=False)\n\n    def forward(self, hidden, encoder_outputs):\n        batch_size = encoder_outputs.shape[1]\n        src_len = encoder_outputs.shape[0]\n        \n        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n        \n        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n        attention = self.v(energy).squeeze(2)\n        \n        return F.softmax(attention, dim=1)\n\nclass Decoder(nn.Module):\n    def __init__(self, output_dim, emb_dim, hidden_dim, n_layers, dropout, attention):\n        super().__init__()\n        self.output_dim = output_dim\n        self.attention = attention\n        self.embedding = nn.Embedding(output_dim, emb_dim)\n        self.lstm = nn.LSTM(hidden_dim * 2 + emb_dim, hidden_dim, n_layers, dropout=dropout)\n        self.fc_out = nn.Linear(hidden_dim * 3 + emb_dim, output_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, input, hidden, cell, encoder_outputs):\n        input = input.unsqueeze(0)\n        embedded = self.dropout(self.embedding(input))\n        \n        a = self.attention(hidden[-1], encoder_outputs)\n        a = a.unsqueeze(1)\n        \n        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n        weighted = torch.bmm(a, encoder_outputs)\n        weighted = weighted.permute(1, 0, 2)\n        \n        rnn_input = torch.cat((embedded, weighted), dim=2)\n        output, (hidden, cell) = self.lstm(rnn_input, (hidden, cell))\n        \n        embedded = embedded.squeeze(0)\n        output = output.squeeze(0)\n        weighted = weighted.squeeze(0)\n        \n        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim=1))\n        \n        return prediction, hidden, cell\n\n\nclass Seq2SeqAttention(nn.Module):\n    def __init__(self, encoder, decoder, device):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n\n    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n        batch_size = src.shape[1]\n        trg_len = trg.shape[0]\n        trg_vocab_size = self.decoder.output_dim\n        \n        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n        encoder_outputs, hidden, cell = self.encoder(src)\n        \n        # Adjust hidden and cell for the decoder\n        hidden = hidden[-self.decoder.lstm.num_layers:]\n        cell = cell[-self.decoder.lstm.num_layers:]\n        \n        input = trg[0,:]\n        \n        for t in range(1, trg_len):\n            output, hidden, cell = self.decoder(input, hidden, cell, encoder_outputs)\n            outputs[t] = output\n            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n            top1 = output.argmax(1)\n            input = trg[t] if teacher_force else top1\n        \n        return outputs","metadata":{"execution":{"iopub.status.busy":"2024-08-28T16:58:10.030710Z","iopub.execute_input":"2024-08-28T16:58:10.031220Z","iopub.status.idle":"2024-08-28T16:58:10.055395Z","shell.execute_reply.started":"2024-08-28T16:58:10.031185Z","shell.execute_reply":"2024-08-28T16:58:10.054488Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport random\nfrom tqdm import tqdm\nfrom nltk.translate.bleu_score import SmoothingFunction\nfrom nltk.translate.bleu_score import sentence_bleu","metadata":{"execution":{"iopub.status.busy":"2024-08-28T16:58:15.441901Z","iopub.execute_input":"2024-08-28T16:58:15.442267Z","iopub.status.idle":"2024-08-28T16:58:16.133721Z","shell.execute_reply.started":"2024-08-28T16:58:15.442231Z","shell.execute_reply":"2024-08-28T16:58:16.132748Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# After the loader section and the new model definition, update the training section:\n\n# Hyperparameters\nINPUT_DIM = len(input_token_index) + 1\nOUTPUT_DIM = len(target_token_index) + 1\nENC_EMB_DIM = 512\nDEC_EMB_DIM = 512\nHIDDEN_DIM = 512\nN_LAYERS = 2\nENC_DROPOUT = 0.5\nDEC_DROPOUT = 0.5\n\n# Model components\nenc = Encoder(INPUT_DIM, ENC_EMB_DIM, HIDDEN_DIM, N_LAYERS, ENC_DROPOUT)\nattn = Attention(HIDDEN_DIM)\ndec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HIDDEN_DIM, N_LAYERS, DEC_DROPOUT, attn)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = Seq2SeqAttention(enc, dec, device).to(device)\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f'The model has {count_parameters(model):,} trainable parameters')\n\n# Initialize weights\ndef init_weights(m):\n    for name, param in m.named_parameters():\n        if 'weight' in name:\n            nn.init.normal_(param.data, mean=0, std=0.01)\n        else:\n            nn.init.constant_(param.data, 0)\n\nmodel.apply(init_weights)\n\n# Optimizer and loss function\noptimizer = torch.optim.Adam(model.parameters())\ncriterion = nn.CrossEntropyLoss(ignore_index=0)  # Assuming 0 is the padding index\n\n# Modify the train_model function\ndef train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10, teacher_forcing_ratio=0.5, clip=1.0):\n    global best_loss, best_model_state, no_improvement_count\n\n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0\n\n        for encoder_input_data, decoder_input_data, decoder_target_data in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n            encoder_input_data = encoder_input_data.transpose(0, 1).to(device)\n            decoder_input_data = decoder_input_data.transpose(0, 1).to(device)\n            decoder_target_data = decoder_target_data.transpose(0, 1).to(device)\n\n            optimizer.zero_grad()\n\n            # Forward pass\n            output = model(encoder_input_data, decoder_input_data, teacher_forcing_ratio)\n\n            # Compute the loss\n            output_dim = output.shape[-1]\n            output = output[1:].view(-1, output_dim)\n            decoder_target_data = decoder_target_data[1:].reshape(-1)\n            loss = criterion(output, decoder_target_data)\n\n            # Backward pass and optimize\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n            optimizer.step()\n\n            total_loss += loss.item()\n\n        avg_loss = total_loss / len(train_loader)\n        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}')\n\n        # Validation step\n        model.eval()\n        val_bleu = evaluate_bleu_score(model, val_loader, reverse_target_token_index)\n        print(f'Epoch [{epoch + 1}/{num_epochs}], Val BLEU Score: {val_bleu:.4f}')\n\n        # Check if the loss has improved\n        if avg_loss < best_loss:\n            best_loss = avg_loss\n            best_model_state = model.state_dict()\n            no_improvement_count = 0\n            print(\"Loss improved, saving model...\")\n        else:\n            no_improvement_count += 1\n            if no_improvement_count >= patience:\n                print(\"No improvement for several epochs, reducing learning rate...\")\n                model.load_state_dict(best_model_state)\n                lr = optimizer.param_groups[0]['lr'] * lr_factor\n                optimizer.param_groups[0]['lr'] = lr\n                print(f\"Learning rate reduced to {lr:.6f}\")\n                no_improvement_count = 0\n\n                if lr < 1e-6:\n                    print(\"Learning rate too low, stopping training.\")\n                    break\n\n    return val_bleu\n\n# Modify the evaluate_bleu_score function\ndef evaluate_bleu_score(model, data_loader, reverse_target_token_index, pad_idx=0):\n    model.eval()\n    total_bleu_score = 0\n    smoothing_function = SmoothingFunction().method4\n    with torch.no_grad():\n        for encoder_input_data, decoder_input_data, decoder_target_data in tqdm(data_loader, desc=\"Evaluating\"):\n            encoder_input_data = encoder_input_data.transpose(0, 1).to(device)\n            decoder_input_data = decoder_input_data.transpose(0, 1).to(device)\n\n            # Get model predictions\n            outputs = model(encoder_input_data, decoder_input_data, teacher_forcing_ratio=0.0)\n\n            # Convert outputs to words\n            predictions = torch.argmax(outputs, dim=2)\n            predicted_sentences = []\n            actual_sentences = []\n\n            for sent in predictions.transpose(0, 1):\n                sentence = []\n                for idx in sent:\n                    if idx.item() == pad_idx:\n                        continue\n                    word = reverse_target_token_index.get(idx.item(), '<UNK>')\n                    sentence.append(word)\n                predicted_sentences.append(sentence)\n\n            for sent in decoder_target_data:\n                sentence = []\n                for idx in sent:\n                    if idx.item() == pad_idx:\n                        continue\n                    word = reverse_target_token_index.get(idx.item(), '<UNK>')\n                    sentence.append(word)\n                actual_sentences.append(sentence)\n\n            # Calculate BLEU score with smoothing\n            for pred_sentence, actual_sentence in zip(predicted_sentences, actual_sentences):\n                total_bleu_score += sentence_bleu([actual_sentence], pred_sentence, smoothing_function=smoothing_function)\n\n    avg_bleu_score = total_bleu_score / len(data_loader.dataset)\n    return avg_bleu_score\n\n# Train the model\nbest_loss = float('inf')\npatience = 3\nlr_factor = 0.5\nnum_epochs = 10\n\n# Tune learning rate on validation data\nlearning_rates = [0.0001]\nbest_bleu = 0\nbest_lr = 0.001\n\nfor lr in learning_rates:\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    print(f'\\nTraining with learning rate: {lr}')\n    val_bleu = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=5, teacher_forcing_ratio=0.5, clip=1.0)\n\n    if val_bleu > best_bleu:\n        best_bleu = val_bleu\n        best_lr = lr\n\nprint(f'Best learning rate: {best_lr} with BLEU Score: {best_bleu:.4f}')\n\n# Train with the best learning rate\noptimizer = optim.Adam(model.parameters(), lr=best_lr)\nval_bleu = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=num_epochs, teacher_forcing_ratio=0.5, clip=1.0)\n\n# Evaluate the model on the test data\ntest_bleu_score = evaluate_bleu_score(model, test_loader, reverse_target_token_index)\nprint(f'BLEU Score on Test Data: {test_bleu_score:.4f}')","metadata":{"execution":{"iopub.status.busy":"2024-08-28T16:58:55.926690Z","iopub.execute_input":"2024-08-28T16:58:55.927046Z","iopub.status.idle":"2024-08-28T17:43:57.286313Z","shell.execute_reply.started":"2024-08-28T16:58:55.927012Z","shell.execute_reply":"2024-08-28T17:43:57.285296Z"},"trusted":true},"outputs":[{"name":"stdout","text":"The model has 27,097,225 trainable parameters\n\nTraining with learning rate: 0.0001\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/5: 100%|██████████| 329/329 [02:36<00:00,  2.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/5], Loss: 5.1311\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 71/71 [00:18<00:00,  3.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/5], Val BLEU Score: 0.1631\nLoss improved, saving model...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5: 100%|██████████| 329/329 [02:40<00:00,  2.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/5], Loss: 4.4009\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 71/71 [00:18<00:00,  3.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/5], Val BLEU Score: 0.1650\nLoss improved, saving model...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5: 100%|██████████| 329/329 [02:40<00:00,  2.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/5], Loss: 4.1280\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 71/71 [00:18<00:00,  3.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/5], Val BLEU Score: 0.1647\nLoss improved, saving model...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5: 100%|██████████| 329/329 [02:40<00:00,  2.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/5], Loss: 3.9579\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 71/71 [00:18<00:00,  3.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/5], Val BLEU Score: 0.1648\nLoss improved, saving model...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5: 100%|██████████| 329/329 [02:40<00:00,  2.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [5/5], Loss: 3.8198\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 71/71 [00:18<00:00,  3.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [5/5], Val BLEU Score: 0.1652\nLoss improved, saving model...\nBest learning rate: 0.0001 with BLEU Score: 0.1652\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10: 100%|██████████| 329/329 [02:40<00:00,  2.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/10], Loss: 3.7537\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 71/71 [00:18<00:00,  3.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/10], Val BLEU Score: 0.1647\nLoss improved, saving model...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10: 100%|██████████| 329/329 [02:40<00:00,  2.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/10], Loss: 3.5755\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 71/71 [00:18<00:00,  3.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/10], Val BLEU Score: 0.1630\nLoss improved, saving model...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10: 100%|██████████| 329/329 [02:40<00:00,  2.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/10], Loss: 3.4365\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 71/71 [00:18<00:00,  3.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/10], Val BLEU Score: 0.1628\nLoss improved, saving model...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10: 100%|██████████| 329/329 [02:40<00:00,  2.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/10], Loss: 3.2993\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 71/71 [00:18<00:00,  3.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/10], Val BLEU Score: 0.1612\nLoss improved, saving model...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10: 100%|██████████| 329/329 [02:40<00:00,  2.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [5/10], Loss: 3.1732\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 71/71 [00:18<00:00,  3.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [5/10], Val BLEU Score: 0.1610\nLoss improved, saving model...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10: 100%|██████████| 329/329 [02:40<00:00,  2.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [6/10], Loss: 3.0354\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 71/71 [00:18<00:00,  3.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [6/10], Val BLEU Score: 0.1590\nLoss improved, saving model...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10: 100%|██████████| 329/329 [02:40<00:00,  2.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [7/10], Loss: 2.9118\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 71/71 [00:18<00:00,  3.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [7/10], Val BLEU Score: 0.1591\nLoss improved, saving model...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10: 100%|██████████| 329/329 [02:41<00:00,  2.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [8/10], Loss: 2.7992\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 71/71 [00:18<00:00,  3.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [8/10], Val BLEU Score: 0.1572\nLoss improved, saving model...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10: 100%|██████████| 329/329 [02:40<00:00,  2.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [9/10], Loss: 2.6960\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 71/71 [00:18<00:00,  3.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [9/10], Val BLEU Score: 0.1548\nLoss improved, saving model...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10: 100%|██████████| 329/329 [02:40<00:00,  2.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [10/10], Loss: 2.5930\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 71/71 [00:18<00:00,  3.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [10/10], Val BLEU Score: 0.1556\nLoss improved, saving model...\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 71/71 [00:18<00:00,  3.84it/s]","output_type":"stream"},{"name":"stdout","text":"BLEU Score on Test Data: 0.1554\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import torch\nimport random\n\n# Function to save model parameters\ndef save_model(model, optimizer, epoch, loss, filename):\n    torch.save({\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'loss': loss,\n    }, filename)\n\n# Save the best model\nsave_model(model, optimizer, num_epochs, best_loss, 'best_model.pth')\nprint(\"Model saved successfully.\")\n\n# Function to load model parameters\ndef load_model(model, optimizer, filename):\n    checkpoint = torch.load(filename)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    epoch = checkpoint['epoch']\n    loss = checkpoint['loss']\n    return model, optimizer, epoch, loss\n\n# To load the model later, you can use:\nloaded_model, loaded_optimizer, loaded_epoch, loaded_loss = load_model(model, optimizer, 'best_model.pth')\nprint(f\"Loaded model from epoch {loaded_epoch} with loss {loaded_loss}\")\n\n ","metadata":{"execution":{"iopub.status.busy":"2024-08-28T17:46:07.135760Z","iopub.execute_input":"2024-08-28T17:46:07.136186Z","iopub.status.idle":"2024-08-28T17:46:07.758796Z","shell.execute_reply.started":"2024-08-28T17:46:07.136144Z","shell.execute_reply":"2024-08-28T17:46:07.757884Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Model saved successfully.\nLoaded model from epoch 10 with loss 2.5930108085591743\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3353098604.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(filename)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Function to translate a sentence\ndef translate_sentence(model, sentence, input_token_index, target_token_index, reverse_target_token_index, max_length_src, max_length_tar, device):\n    model.eval()\n    with torch.no_grad():\n        # Tokenize and pad the input sentence\n        encoder_input = torch.zeros(max_length_src, 1, dtype=torch.long).to(device)\n        for t, word in enumerate(sentence.split()[:max_length_src]):\n            encoder_input[t, 0] = input_token_index.get(word, 0)\n        \n        # Generate the translation\n        decoder_input = torch.tensor([[target_token_index['START_']]]).to(device)\n        decoded_sentence = []\n        \n        encoder_outputs, hidden, cell = model.encoder(encoder_input)\n        \n        # Adjust hidden and cell for the decoder by combining the bidirectional outputs\n        hidden = hidden[-model.decoder.lstm.num_layers:] + hidden[:model.decoder.lstm.num_layers]\n        cell = cell[-model.decoder.lstm.num_layers:] + cell[:model.decoder.lstm.num_layers]\n        \n        for _ in range(max_length_tar):\n            output, hidden, cell = model.decoder(decoder_input.squeeze(0), hidden, cell, encoder_outputs)\n            predicted_token = output.argmax(1).item()\n            decoded_word = reverse_target_token_index.get(predicted_token, '<UNK>')\n            decoded_sentence.append(decoded_word)\n            \n            if decoded_word == '_END':\n                break\n            \n            decoder_input = torch.tensor([[predicted_token]], device=device)\n    \n    return ' '.join(decoded_sentence)\n\n# Evaluate BLEU score on test set\ntest_bleu_score = evaluate_bleu_score(model, test_loader, reverse_target_token_index)\nprint(f'BLEU Score on Test Data: {test_bleu_score:.4f}')\n\n# Show random translation examples\nnum_examples = 5\nrandom_indices = random.sample(range(len(X_test)), num_examples)\nfor idx in random_indices:\n    input_sentence = X_test.iloc[idx]\n    target_sentence = y_test.iloc[idx]\n    translated_sentence = translate_sentence(model, input_sentence, input_token_index, target_token_index, reverse_target_token_index, max_length_src, max_length_tar, device)\n    \n    print(f\"\\nExample {idx + 1}:\")\n    print(f\"Input (English): {input_sentence}\")\n    print(f\"Target (Hindi): {target_sentence}\")\n    print(f\"Model Output: {translated_sentence}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-28T11:08:26.316089Z","iopub.execute_input":"2024-08-28T11:08:26.316824Z","iopub.status.idle":"2024-08-28T11:08:45.083235Z","shell.execute_reply.started":"2024-08-28T11:08:26.316787Z","shell.execute_reply":"2024-08-28T11:08:45.082253Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Evaluating: 100%|██████████| 71/71 [00:18<00:00,  3.80it/s]","output_type":"stream"},{"name":"stdout","text":"BLEU Score on Test Data: 0.1180\n\nExample 1643:\nInput (English): move a onto the red joker\nTarget (Hindi): START_  a को लाल जोकर पर ले जाएँ _END\nModel Output: को लाल पर पर ले जाएँ _END\n\nExample 4076:\nInput (English): file url to remove\nTarget (Hindi): START_ फ़ाइल url को हटाएँः _END\nModel Output: url हटाएँः हटाएँः _END\n\nExample 3956:\nInput (English):  edit\nTarget (Hindi): START_ संपादन e _END\nModel Output: करेंmenu _END\n\nExample 3572:\nInput (English):  properties\nTarget (Hindi): START_ गुण p _END\nModel Output: गुण _END\n\nExample 1288:\nInput (English): target name\nTarget (Hindi): START_ परियोजना नामः _END\nModel Output: नामः _END\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Show random translation examples with at least 10 tokens\nnum_examples = 5\nfiltered_indices = [idx for idx, sentence in enumerate(X_test) if len(sentence.split()) >= 10]\nrandom_indices = random.sample(filtered_indices, num_examples)\n\nfor idx in random_indices:\n    input_sentence = X_test.iloc[idx]\n    target_sentence = y_test.iloc[idx]\n    translated_sentence = translate_sentence(model, input_sentence, input_token_index, target_token_index, reverse_target_token_index, max_length_src, max_length_tar, device)\n    \n    print(f\"\\nExample {idx + 1}:\")\n    print(f\"Input (English): {input_sentence}\")\n    print(f\"Target (Hindi): {target_sentence}\")\n    print(f\"Model Output: {translated_sentence}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-28T11:08:57.473009Z","iopub.execute_input":"2024-08-28T11:08:57.473774Z","iopub.status.idle":"2024-08-28T11:08:57.561388Z","shell.execute_reply.started":"2024-08-28T11:08:57.473732Z","shell.execute_reply":"2024-08-28T11:08:57.560413Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\nExample 207:\nInput (English): place the jack of diamonds next to the ten of diamonds\nTarget (Hindi): START_ ईंट का गुलाम के बगल में ईंट का दहला को रखें _END\nModel Output: का गुलाम के बगल में ईंट का दहला को रखें _END\n\nExample 1410:\nInput (English): project name it can contain spaces by example gnu autoconf\nTarget (Hindi): START_ परियोजना नाम द्वारा उदाहरण _END\nModel Output: नाम नहीं नाम उदाहरण से _END\n\nExample 1680:\nInput (English): a list of uris for partitions to be excluded from scanning\nTarget (Hindi): START_ स्कैनिंग से अलग करने के लिए विभाजन के लिए uris की सूचि _END\nModel Output: से अलग के लिए द्वारा द्वारा से के लिए की सूचि _END\n\nExample 1441:\nInput (English): place the queen of spades next to the jack of spades\nTarget (Hindi): START_ हुकुम की बेगम के बगल में हुकुम के गुलाम को रखें _END\nModel Output: की बेगम के बगल में हुकुम के गुलाम को रखें _END\n\nExample 1592:\nInput (English): cannot compile s no compile rule defined for this file type\nTarget (Hindi): START_ से नहीं नियम के लिए यह फ़ाइल क़िस्म _END\nModel Output: से नियम के लिए लिए लिए फ़ाइल क़िस्म _END\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"def beam_search(model, sentence, input_token_index, target_token_index, reverse_target_token_index, max_length_src, max_length_tar, device, beam_size=3):\n    model.eval()\n    with torch.no_grad():\n        # Tokenize and pad the input sentence\n        encoder_input = torch.zeros(max_length_src, 1, dtype=torch.long).to(device)\n        for t, word in enumerate(sentence.split()[:max_length_src]):\n            encoder_input[t, 0] = input_token_index.get(word, 0)\n        \n        # Encode the input sentence\n        encoder_outputs, hidden, cell = model.encoder(encoder_input)\n        \n        # Adjust hidden and cell for the decoder\n        hidden = hidden[-model.decoder.lstm.num_layers:] + hidden[:model.decoder.lstm.num_layers]\n        cell = cell[-model.decoder.lstm.num_layers:] + cell[:model.decoder.lstm.num_layers]\n        \n        # Initialize the beam\n        beam = [(torch.tensor([[target_token_index['START_']]], device=device), hidden, cell, [target_token_index['START_']], 0)]\n        \n        for _ in range(max_length_tar):\n            candidates = []\n            for decoder_input, hidden, cell, sequence, score in beam:\n                if sequence[-1] == target_token_index['_END']:\n                    candidates.append((decoder_input, hidden, cell, sequence, score))\n                    continue\n                \n                output, new_hidden, new_cell = model.decoder(decoder_input.squeeze(0), hidden, cell, encoder_outputs)\n                log_probs = F.log_softmax(output, dim=1)\n                top_log_probs, top_indices = log_probs.topk(beam_size)\n                \n                for i in range(beam_size):\n                    token = top_indices[0][i].item()\n                    new_score = score + top_log_probs[0][i].item()\n                    new_sequence = sequence + [token]\n                    candidates.append((torch.tensor([[token]], device=device), new_hidden, new_cell, new_sequence, new_score))\n            \n            # Select the top beam_size candidates\n            beam = sorted(candidates, key=lambda x: x[4], reverse=True)[:beam_size]\n            \n            # Check if all beams have ended\n            if all(sequence[-1] == target_token_index['_END'] for _, _, _, sequence, _ in beam):\n                break\n    \n    # Return the best sequence or an empty list if no valid translation\n    if beam:\n        best_sequence = beam[0][3]\n        decoded_sentence = [reverse_target_token_index.get(token, '<UNK>') for token in best_sequence]\n        return ' '.join(decoded_sentence[1:-1])  # Exclude the 'START_' and '_END' tokens\n    else:\n        return ''","metadata":{"execution":{"iopub.status.busy":"2024-08-25T15:22:57.576655Z","iopub.execute_input":"2024-08-25T15:22:57.577270Z","iopub.status.idle":"2024-08-25T15:22:57.591245Z","shell.execute_reply.started":"2024-08-25T15:22:57.577230Z","shell.execute_reply":"2024-08-25T15:22:57.590289Z"},"trusted":true},"outputs":[],"execution_count":39},{"cell_type":"code","source":"def evaluate_bleu_score_beam_search(model, data_loader, input_token_index, target_token_index, reverse_target_token_index, reverse_input_token_index, max_length_src, max_length_tar, device):\n    model.eval()\n    references = []\n    hypotheses = []\n    smoothing_function = SmoothingFunction().method4\n\n    with torch.no_grad():\n        for encoder_input_data, decoder_input_data, decoder_target_data in data_loader:\n            encoder_input_data = encoder_input_data.transpose(0, 1).to(device)\n            decoder_target_data = decoder_target_data.transpose(0, 1).to(device)\n            \n            for i in range(encoder_input_data.size(1)):\n                try:\n                    input_sentence = ' '.join([reverse_input_token_index.get(idx.item(), '<UNK>') for idx in encoder_input_data[:, i] if idx.item() != 0])\n                    target_sentence = ' '.join([reverse_target_token_index.get(idx.item(), '<UNK>') for idx in decoder_target_data[:, i] if idx.item() != 0])\n                    \n                    translated_sentence = beam_search(model, input_sentence, input_token_index, target_token_index, reverse_target_token_index, max_length_src, max_length_tar, device)\n                    \n                    if translated_sentence:\n                        references.append([target_sentence.split()])\n                        hypotheses.append(translated_sentence.split())\n                except Exception as e:\n                    print(f\"Error processing sentence {i}: {str(e)}\")\n                    continue\n\n    if references and hypotheses:\n        bleu_score = corpus_bleu(references, hypotheses, smoothing_function=smoothing_function)\n    else:\n        bleu_score = 0.0\n    return bleu_score","metadata":{"execution":{"iopub.status.busy":"2024-08-25T15:23:00.628266Z","iopub.execute_input":"2024-08-25T15:23:00.629003Z","iopub.status.idle":"2024-08-25T15:23:00.638741Z","shell.execute_reply.started":"2024-08-25T15:23:00.628963Z","shell.execute_reply":"2024-08-25T15:23:00.637867Z"},"trusted":true},"outputs":[],"execution_count":40},{"cell_type":"code","source":"# Evaluate BLEU score on test set using beam search\ntest_bleu_score_beam = evaluate_bleu_score_beam_search(\n    model=model,\n    data_loader=test_loader,\n    input_token_index=input_token_index,\n    target_token_index=target_token_index,\n    reverse_target_token_index=reverse_target_token_index,\n    reverse_input_token_index=reverse_input_token_index,\n    max_length_src=max_length_src,\n    max_length_tar=max_length_tar,\n    device=device\n)\n\nprint(f'BLEU Score on Test Data (Beam Search): {test_bleu_score_beam:.4f}')","metadata":{"execution":{"iopub.status.busy":"2024-08-25T15:23:04.979550Z","iopub.execute_input":"2024-08-25T15:23:04.980351Z","iopub.status.idle":"2024-08-25T15:23:07.035498Z","shell.execute_reply.started":"2024-08-25T15:23:04.980302Z","shell.execute_reply":"2024-08-25T15:23:07.034507Z"},"trusted":true},"outputs":[{"name":"stdout","text":"BLEU Score on Test Data (Beam Search): 0.0766\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"import random\n\ndef translate_random_sentences(model, X_test, y_test, input_token_index, target_token_index, reverse_target_token_index, max_length_src, max_length_tar, device, num_examples=5):\n    model.eval()\n    random_indices = random.sample(range(len(X_test)), num_examples)\n    \n    print(\"\\nRandom Translation Examples using Beam Search:\")\n    print(\"----------------------------------------------\")\n    \n    for idx in random_indices:\n        input_sentence = X_test.iloc[idx]\n        target_sentence = y_test.iloc[idx]\n        \n        # Perform beam search translation\n        translated_sentence = beam_search(model, input_sentence, input_token_index, target_token_index, reverse_target_token_index, max_length_src, max_length_tar, device)\n        \n        print(f\"\\nExample {idx + 1}:\")\n        print(f\"Input (English): {input_sentence}\")\n        print(f\"Target (Hindi): {target_sentence}\")\n        print(f\"Model Output (Beam Search): {translated_sentence}\")\n        \n        # Optionally, you can compute BLEU score for this specific translation\n        reference = [target_sentence.split()]\n        hypothesis = translated_sentence.split()\n        bleu_score = sentence_bleu(reference, hypothesis, smoothing_function=SmoothingFunction().method1)\n        print(f\"BLEU Score: {bleu_score:.4f}\")\n\n# Use the function\ntranslate_random_sentences(\n    model=model,\n    X_test=X_test,\n    y_test=y_test,\n    input_token_index=input_token_index,\n    target_token_index=target_token_index,\n    reverse_target_token_index=reverse_target_token_index,\n    max_length_src=max_length_src,\n    max_length_tar=max_length_tar,\n    device=device,\n    num_examples=5  # You can change this number to display more or fewer examples\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-25T15:24:20.016615Z","iopub.execute_input":"2024-08-25T15:24:20.017260Z","iopub.status.idle":"2024-08-25T15:24:20.104029Z","shell.execute_reply.started":"2024-08-25T15:24:20.017220Z","shell.execute_reply":"2024-08-25T15:24:20.103147Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\nRandom Translation Examples using Beam Search:\n----------------------------------------------\n\nExample 18:\nInput (English): refresh node\nTarget (Hindi): START_ आसंधि नोड को ताजा करें n _END\nModel Output (Beam Search): गए नहीं\nBLEU Score: 0.0000\n\nExample 94:\nInput (English): not implemented\nTarget (Hindi): START_ क्रियान्वित नहीं हुआ है _END\nModel Output (Beam Search): गए को को\nBLEU Score: 0.0000\n\nExample 88:\nInput (English): a list of plugins that are disabled by default\nTarget (Hindi): START_ उन प्लगइनों की सूची जिन्हें डिफोल्ट रूप से निष्क्रिय किया गया है _END\nModel Output (Beam Search): प्लगइनों की सूची जिन्हें डिफोल्ट रूप निष्क्रिय निष्क्रिय किया गया\nBLEU Score: 0.4411\n\nExample 146:\nInput (English): key\nTarget (Hindi): START_ कुंजी _END\nModel Output (Beam Search): \nBLEU Score: 0.0000\n\nExample 123:\nInput (English): col lection\nTarget (Hindi): START_ संग्रह _END\nModel Output (Beam Search): \nBLEU Score: 0.0000\n","output_type":"stream"}],"execution_count":42}]}